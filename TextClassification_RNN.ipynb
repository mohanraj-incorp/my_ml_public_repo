{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMGXXgNF7k61"
   },
   "source": [
    "# __Text Classification Using RNN__\n",
    "\n",
    "Let's see how to classify the text using RNN (Recurrent Neural Network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 443,
     "status": "ok",
     "timestamp": 1718957094758,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "RME2Qywv_p7Z",
    "outputId": "80858f38-cd2a-489f-920b-c1286451af02"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sowmya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuJT1hcu92er"
   },
   "source": [
    "## Steps to be followed:\n",
    "\n",
    "1. Import the libraries\n",
    "2. Define the hyperparameter\n",
    "3. Preprocess the data and print the lengths of the labels and article lists\n",
    "4. Split the data into training and validation sets\n",
    "5. Initialize a tokenizer and fit it to the training articles\n",
    "6. Convert the training articles into sequences using the tokenizer\n",
    "7. Pad the sequence\n",
    "8. Print the length of validation sequences and the shape of validation padded\n",
    "9. Train the model\n",
    "10. Compile the model\n",
    "11. Plot the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKxl5ljN-IKH"
   },
   "source": [
    "### Step 1: Import the libraries\n",
    "- Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1718957102948,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "qLg1HxV27k64"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cvQu2uW_AF0"
   },
   "source": [
    "### Step 2: Define the hyperparameter\n",
    "- Set the value of __vocab_size__ to __5000__, representing the size of the vocabulary.\n",
    "- Set the value of __embedding_dim__ to __64__, specifying the dimensionality of the word embeddings.\n",
    "- Set the value of __max_length__ to __200__, indicating the maximum length of input sequences.\n",
    "- Set the value of __padding_type__ to __post__, specifying that padding should be added at the end of sequences.\n",
    "- Set the value of __trunc_type__ to __post__, indicating that truncation should be applied at the end of sequences.\n",
    "- Set the value of __oov_tok__ to __OOV__, representing the token to be used for out-of-vocabulary words.\n",
    "- Set the value of __training_portion__ to __0.8__, representing the proportion of data to be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 460,
     "status": "ok",
     "timestamp": 1718957106585,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "BecOwkf37k66"
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "embedding_dim = 64\n",
    "max_length = 200\n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "training_portion = .8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5682GVs7k66"
   },
   "source": [
    "### Step 3: Preprocess the data and print the lengths of the labels and articles lists\n",
    "\n",
    "- Define two empty lists, articles, and labels to store the articles and labels, respectively.\n",
    "- Read the contents of the **bbc-text.csv** file using csv.reader and iterate through each row.\n",
    "- Extract the labels from the first column of each row and append it to the labels list.\n",
    "- Process the article from the second column by removing stopwords and replacing consecutive spaces with a single space, and then append it to the article list.\n",
    "- Print the lengths of the labels and article lists to display the number of labels and processed articles, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1718957147629,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "YOrfsrUe7k67",
    "outputId": "9181c93c-8cf7-41a7-9d25-e74cdc405e0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n",
      "2225\n"
     ]
    }
   ],
   "source": [
    "articles = []\n",
    "labels = []\n",
    "#\"D:\\SimpliLearn_2024_2025\\Deep_Learning_with_Keras_and_TensorFlow_ILT_Material\\0.2_LVC\\Datasets\\Lesson_11_Recurrent_Neural_Networks\\bbc-text.csv\"\n",
    "with open(\"bbc-text_1742742988916.csv\", 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)  # skip header\n",
    "    for row in reader:\n",
    "        label, article = row\n",
    "        article = ' '.join([word for word in article.split() if word not in STOPWORDS])\n",
    "        articles.append(article)\n",
    "        labels.append(label)\n",
    "print(len(labels))\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qtk-qqt67k67"
   },
   "source": [
    "__Observations:__\n",
    "- There are only **2,225** articles in the data.\n",
    "- Then, we split into a training set and a validation set, according to the parameter we set earlier, 80% for training, and 20% for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C72gvCwGSa0k"
   },
   "source": [
    "### Step 4: Split the data into training and validation sets\n",
    "- Calculate the **train_size** by multiplying the length of the article list with __training_portion__ and converting it to an integer.\n",
    "- Create **train_articles** by slicing the article list from index **0** to **train_size.**\n",
    "- Create **train_labels** by slicing the labels list from index **0** to **train_size.**\n",
    "- Create validation_articles by slicing the articles list from **train_size** onward.\n",
    "- Create **validation_labels** by slicing the labels list from **train_size** onward.\n",
    "- Print the **train_size** to display the calculated value.\n",
    "\n",
    "- The lengths of **train_articles**, **train_labels**, **validation_articles**, and **validation_labels** represent the number of items in each list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1718958991506,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "9iLpsnaO8BXW",
    "outputId": "9f507d56-3ba8-4483-a82f-78f2eac7dd7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sport', 'tech', 'business', 'politics', 'entertainment'}\n"
     ]
    }
   ],
   "source": [
    "print(set(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3bHoV3B8Gw6"
   },
   "source": [
    "__Observation:__\n",
    "\n",
    "- The output is a set containing the unique labels: 'business', 'tech', 'entertainment', 'politics', and 'sport'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1718958768116,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "van-NAJD684_"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert labels to integers using LabelEncoder which automatically makes them 0-indexed\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 496,
     "status": "ok",
     "timestamp": 1718959094961,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "nhVRtDja8iOl",
    "outputId": "6e894287-b7bf-40ba-d0e4-bd04df9d7f51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 5\n"
     ]
    }
   ],
   "source": [
    "# Check unique labels and adjust the model's last Dense layer\n",
    "num_classes = len(np.unique(labels_encoded))\n",
    "print(\"Number of unique labels:\", num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 407,
     "status": "ok",
     "timestamp": 1718958859393,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "yQc3ljSb1NBH",
    "outputId": "6c1754a1-50dd-4206-ab56-5a20ce338109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1780\n",
      "1780\n",
      "1780\n",
      "445\n",
      "445\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "train_size = int(len(articles) * training_portion)\n",
    "train_articles = articles[:train_size]\n",
    "training_label_seq = labels_encoded[:train_size]\n",
    "\n",
    "validation_articles = articles[train_size:]\n",
    "validation_label_seq = labels_encoded[train_size:]\n",
    "\n",
    "\n",
    "print(train_size)\n",
    "print(len(train_articles))\n",
    "print(len(training_label_seq))\n",
    "print(len(validation_articles))\n",
    "print(len(validation_label_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dx4Pb61WT2F3"
   },
   "source": [
    "__Observations:__\n",
    "- The value of **train_size** is calculated based on the proportion of training data.\n",
    "- The lengths of **train_articles**, **training_label_seq**, **validation_articles**, and **validation_label_seq** represent the number of items in each list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSR2G2lrWrue"
   },
   "source": [
    "### Step 5: Initialize a tokenizer and fit it to the training articles\n",
    "\n",
    "- Initialize a **Tokenizer** object named tokenizer with the specified parameters: **num_words** representing the vocabulary size and **oov_token** representing the out-of-vocabulary token.\n",
    "- Fit the tokenizer on the training articles **(train_articles)** using the **fit_on_texts** method.\n",
    "\n",
    "  `fit_on_texts`: Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it a sample sentence like\n",
    "\n",
    "    `The cat sat on the mat.`\n",
    "\n",
    "  It will create a dictionary, where every word gets a unique integer value. 0 is reserved for padding.\n",
    "      word_index[\"the\"] = 1; word_index[\"cat\"] = 2.\n",
    "\n",
    "- This step updates the tokenizer's internal word index based on the words in the training articles.\n",
    "- Assign the word index obtained from the tokenizer to the variable **word_index.**\n",
    "- Extract the first 10 items from the word_index dictionary\n",
    "- Print the resulting dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 1056,
     "status": "ok",
     "timestamp": 1718957940049,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "2JrnZlTL7k68"
   },
   "outputs": [],
   "source": [
    "# Initialize and fit tokenizer on training data only\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_articles)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27270"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 417,
     "status": "ok",
     "timestamp": 1718957944718,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "7lhT9_-f7k68",
    "outputId": "bcd6c65b-07a8-4dd5-d893-05ae09d11495"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'said': 2,\n",
       " 'mr': 3,\n",
       " 'would': 4,\n",
       " 'year': 5,\n",
       " 'also': 6,\n",
       " 'people': 7,\n",
       " 'new': 8,\n",
       " 'us': 9,\n",
       " 'one': 10}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(word_index.items())[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQ_XTmT_eU7b"
   },
   "source": [
    "__Observations:__\n",
    "- The code prints a dictionary containing the first 10 items from the word_index dictionary.\n",
    "- These items represent a subset of the word-to-index mappings generated by the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTDip0vPe30D"
   },
   "source": [
    "### Step 6: Convert the training articles into sequences using the tokenizer\n",
    "- Convert the training articles **(train_articles)** into sequences of numbers using the `texts_to_sequences` method of the tokenizer object and assign the result to `train_sequences` variable\n",
    "    \n",
    "- Print the sequence representation of the 11th training article (index 10) by accessing **train_sequences[10].**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1718958069956,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "qaNkkgzP7k69",
    "outputId": "57f322df-cae2-4777-bfad-84194fec1796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2430, 1, 225, 4991, 22, 641, 586, 225, 4991, 1, 1, 1660, 1, 1, 2430, 22, 564, 1, 1, 140, 278, 1, 140, 278, 796, 822, 661, 2305, 1, 1144, 1691, 1, 1718, 4992, 1, 1, 1, 1, 1, 4733, 1, 1, 122, 4510, 1, 2, 2873, 1503, 352, 4734, 1, 52, 341, 1, 352, 2170, 3958, 41, 22, 3792, 1, 1, 1, 1, 542, 1, 1, 1, 834, 631, 2365, 347, 4735, 1, 365, 22, 1, 787, 2366, 1, 4298, 138, 10, 1, 3663, 681, 3531, 1, 22, 1, 414, 822, 661, 1, 90, 13, 633, 1, 225, 4991, 1, 598, 1, 1691, 1021, 1, 4993, 807, 1861, 117, 1, 1, 1, 2973, 22, 1, 99, 278, 1, 1604, 4994, 542, 492, 1, 1441, 4736, 778, 1319, 1, 1858, 10, 33, 641, 319, 1, 62, 478, 564, 301, 1504, 22, 479, 1, 1, 1663, 1, 797, 1, 3065, 1, 1363, 6, 1, 2430, 564, 22, 2970, 4730, 1, 1, 1, 1, 1, 850, 39, 1822, 674, 297, 26, 979, 1, 882, 22, 361, 22, 13, 301, 1504, 1341, 374, 20, 63, 883, 1096, 4299, 247]\n"
     ]
    }
   ],
   "source": [
    "train_sequences  = tokenizer.texts_to_sequences(train_articles)\n",
    "\n",
    "# train_sequences is a list of lists\n",
    "print(train_sequences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin cheers anti-nazi film german movie anti-nazi resistance heroine drawn loud applause berlin film festival. sophie scholl - final days portrays final days member white rose movement. scholl 21 arrested beheaded brother hans 1943 distributing leaflets condemning abhorrent tyranny adolf hitler. director marc rothemund said: feeling responsibility keep legacy scholls going. must somehow keep ideas alive added. film drew transcripts gestapo interrogations scholl trial preserved archive communist east germany secret police. discovery inspiration behind film rothemund worked closely surviving relatives including one scholl sisters ensure historical accuracy film. scholl members white rose resistance group first started distributing anti-nazi leaflets summer 1942. arrested dropped leaflets munich university calling day reckoning adolf hitler regime. film focuses six days scholl arrest intense trial saw scholl initially deny charges ended defiant appearance. one three german films vying top prize festival. south african film version bizet tragic opera carmen shot cape town xhosa language also premiered berlin festival. film entitled u-carmen ekhayelitsha carmen khayelitsha township story set. performed 40-strong music theatre troupe debut film performance. film first south african feature 25 years second nominated golden bear award.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_articles[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7153"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['resistance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1780"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__Jgr_cdfR_6"
   },
   "source": [
    "__Observation:__\n",
    "- The code prints the sequence representation of the 11th training article (index 10) in the **train_sequences** list.\n",
    "- The output is a list of integers, where each integer represents the index of a word in the tokenizer's word index vocabulary that corresponds to a word in the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E60-EoRMh58g"
   },
   "source": [
    "### Step 7: Pad the Sequence\n",
    "- Pad the sequences in **train_sequences** using the **pad_sequences** function. It is done so that every sequence has the same length.\n",
    "- Set the maximum length of the padded sequences to **max_length.**\n",
    "- Specify the padding type as **padding_type** and the truncation type as **trunc_type.**\n",
    "- Assign the padded sequences to the variable **train_padded.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 459,
     "status": "ok",
     "timestamp": 1718958085473,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "msiF0l767k69"
   },
   "outputs": [],
   "source": [
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1718956711468,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "EQbLYMWl7k6-",
    "outputId": "57cd5e7d-8a2d-4016-d41f-857decf7d454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2430    1  225 4991   22  641  586  225 4991    1    1 1660    1    1\n",
      " 2430   22  564    1    1  140  278    1  140  278  796  822  661 2305\n",
      "    1 1144 1691    1 1718 4992    1    1    1    1    1 4733    1    1\n",
      "  122 4510    1    2 2873 1503  352 4734    1   52  341    1  352 2170\n",
      " 3958   41   22 3792    1    1    1    1  542    1    1    1  834  631\n",
      " 2365  347 4735    1  365   22    1  787 2366    1 4298  138   10    1\n",
      " 3663  681 3531    1   22    1  414  822  661    1   90   13  633    1\n",
      "  225 4991    1  598    1 1691 1021    1 4993  807 1861  117    1    1\n",
      "    1 2973   22    1   99  278    1 1604 4994  542  492    1 1441 4736\n",
      "  778 1319    1 1858   10   33  641  319    1   62  478  564  301 1504\n",
      "   22  479    1    1 1663    1  797    1 3065    1 1363    6    1 2430\n",
      "  564   22 2970 4730    1    1    1    1    1  850   39 1822  674  297\n",
      "   26  979    1  882   22  361   22   13  301 1504 1341  374   20   63\n",
      "  883 1096 4299  247    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_padded[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kR1yorJLkMpI"
   },
   "source": [
    "__Observation:__\n",
    "- The code prints the padded sequence representation of the 11th training article.\n",
    "- The output is a list of integers representing the word indices of the corresponding words in the article, after applying padding to ensure a consistent length (max_length) for all sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QD6M00flHAm"
   },
   "source": [
    "### Step 8: Print the length of validation sequences and the shape of validation padded\n",
    "- Convert the validation articles into sequences using the tokenizer and pad the sequences to a maximum length. Assign the result to **validation_padded.**\n",
    "- Print the length of **validation_sequences** and the shape of **validation_padded.**\n",
    "- Create a tokenizer for the labels and fit it on the labels list.\n",
    "- Convert the training and validation labels into sequences using the label tokenizer and store the results in **training_label_seq** and **validation_label_seq** as NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 448,
     "status": "ok",
     "timestamp": 1718958176381,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "fpgc3k9y7k6-",
    "outputId": "95c22c7a-28a9-489e-d7bc-5c110f607cfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445\n",
      "(445, 200)\n"
     ]
    }
   ],
   "source": [
    "validation_sequences = tokenizer.texts_to_sequences(validation_articles)\n",
    "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print(len(validation_sequences))\n",
    "print(validation_padded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFFWDC12mo1e"
   },
   "source": [
    "__Observations:__\n",
    "- The length of **validation_sequences**, indicating the number of sequences in the validation set.\n",
    "- The shape of **validation_padded**, representing the dimensions of the padded validation sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1718958911063,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "Im7CgB8X6hme",
    "outputId": "bc24267b-fc84-495d-d991-b7a18ea727b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique training labels: [0 1 2 3 4]\n",
      "Unique validation labels: [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Confirm that labels are correctly zero-indexed\n",
    "print(\"Unique training labels:\", np.unique(training_label_seq))\n",
    "print(\"Unique validation labels:\", np.unique(validation_label_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aefTdVG3ohC2"
   },
   "source": [
    "### Step 9: Train the model\n",
    "- Create a sequential model using **tf.keras.Sequential().**\n",
    "- Add an embedding layer to the model with the specified vocabulary size **(vocab_size)** and embedding dimension **(embedding_dim).**\n",
    "- Add a bidirectional SimpleRNN layer to the model with the same embedding dimension.\n",
    "- Add a dense layer to the model with the embedding dimension as the number of units and **relu** activation function.\n",
    "- Add a dense layer with `num_classes` which represents the number of unique classes/labels and the **softmax** activation function to the model.\n",
    "- Print a summary of the model's architecture using **model.summary().**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 66260,
     "status": "ok",
     "timestamp": 1718960137383,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "YAOXrQfV-tKC",
    "outputId": "e23d56b3-9057-4e1b-face-6f6a2a339de4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "56/56 - 7s - 118ms/step - accuracy: 0.2084 - loss: 1.6326 - val_accuracy: 0.2652 - val_loss: 1.5553\n",
      "Epoch 2/10\n",
      "56/56 - 3s - 61ms/step - accuracy: 0.3247 - loss: 1.5014 - val_accuracy: 0.3596 - val_loss: 1.4989\n",
      "Epoch 3/10\n",
      "56/56 - 4s - 66ms/step - accuracy: 0.4236 - loss: 1.3961 - val_accuracy: 0.3416 - val_loss: 1.4577\n",
      "Epoch 4/10\n",
      "56/56 - 4s - 65ms/step - accuracy: 0.4904 - loss: 1.1857 - val_accuracy: 0.5101 - val_loss: 1.0970\n",
      "Epoch 5/10\n",
      "56/56 - 4s - 63ms/step - accuracy: 0.6545 - loss: 0.8628 - val_accuracy: 0.5685 - val_loss: 1.0514\n",
      "Epoch 6/10\n",
      "56/56 - 3s - 61ms/step - accuracy: 0.7747 - loss: 0.6004 - val_accuracy: 0.7236 - val_loss: 0.7250\n",
      "Epoch 7/10\n",
      "56/56 - 4s - 63ms/step - accuracy: 0.7938 - loss: 0.5404 - val_accuracy: 0.6404 - val_loss: 0.8550\n",
      "Epoch 8/10\n",
      "56/56 - 4s - 68ms/step - accuracy: 0.9028 - loss: 0.3140 - val_accuracy: 0.7663 - val_loss: 0.6402\n",
      "Epoch 9/10\n",
      "56/56 - 4s - 63ms/step - accuracy: 0.9579 - loss: 0.1549 - val_accuracy: 0.8067 - val_loss: 0.5338\n",
      "Epoch 10/10\n",
      "56/56 - 4s - 64ms/step - accuracy: 0.9781 - loss: 0.0899 - val_accuracy: 0.8404 - val_loss: 0.4957\n"
     ]
    }
   ],
   "source": [
    "model_dropout = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(embedding_dim)),\n",
    "    tf.keras.layers.Dropout(0.5),  # Add dropout\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),  # Add another dropout layer\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model_dropout.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "num_epochs = 10\n",
    "history = model_dropout.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout.save('my_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1132,
     "status": "ok",
     "timestamp": 1718960376588,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "mlW_TEa4AI_1",
    "outputId": "caab2def-b0ba-4645-b636-f3a3e873c80d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 - 0s - 12ms/step - accuracy: 0.8404 - loss: 0.4957\n",
      "Validation loss: 0.4957028031349182\n",
      "Validation accuracy: 0.8404494524002075\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_loss, val_accuracy = model_dropout.evaluate(validation_padded, validation_label_seq, verbose=2)\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "print(f\"Validation accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4525,
     "status": "ok",
     "timestamp": 1718960384907,
     "user": {
      "displayName": "Aleena Raj",
      "userId": "16635257578699511263"
     },
     "user_tz": -330
    },
    "id": "VoBsapTPAI_1",
    "outputId": "bde45f5e-8c92-4ffc-bfd0-7d0ba47adf40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 - 1s - 11ms/step - accuracy: 0.9972 - loss: 0.0111\n",
      "Training loss: 0.01105344295501709\n",
      "Training accuracy: 0.9971910119056702\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the training set\n",
    "train_loss, train_accuracy = model_dropout.evaluate(train_padded, training_label_seq, verbose=2)\n",
    "print(f\"Training loss: {train_loss}\")\n",
    "print(f\"Training accuracy: {train_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ARfa2toBhOa"
   },
   "source": [
    "__Observation__\n",
    "\n",
    "Here, you can see that the validation accuracy has increased to ~77%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VMnjoi2CNhr"
   },
   "source": [
    "Now lets explore one more step to address overfitting called **Early Stopping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flvLADEVE365"
   },
   "source": [
    "**Explanation of Early Stopping Parameters:**\n",
    "\n",
    "`monitor`: This is the metric to be monitored, usually 'val_loss' or 'val_accuracy'.\n",
    "\n",
    "`patience`: The number of epochs to continue training without improvement in the monitored metric. After this number, training stops.\n",
    "\n",
    "`restore_best_weights`: When set to True, the model weights are reverted to those that achieved the best value of the monitored metric. This is useful because even after the condition for patience is met, the best model may have occurred in an earlier epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
